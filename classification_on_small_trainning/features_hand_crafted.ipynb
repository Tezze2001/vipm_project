{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "\n",
    "project_path = os.path.abspath(\"../code\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "project_path = os.path.abspath(\"../networks\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "from models import *\n",
    "from vipm_features import *\n",
    "import vipm_costants as CONST\n",
    "from vipm_pipeline import *\n",
    "from dataset import *\n",
    "\n",
    "def load_csv(csv_path):\n",
    "    data = pd.read_csv(csv_path, header=None, names=['image_name', 'label'])\n",
    "    return data['image_name'].tolist(), data['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = CONST.SMALL_TRAINING_NAME_PATH\n",
    "csv_unlabeled = CONST.TRAINING_NAME_PATH\n",
    "csv_test = CONST.TESTING_NAME_PATH\n",
    "csv_test_deg = CONST.DEG_TESTING_NAME_PATH\n",
    "indir_train = CONST.SMALL_TRAINING_PATH  # Modifica in base alla posizione delle immagini\n",
    "indir_test = CONST.TEST_PATH  # Modifica in base alla posizione delle immagini\n",
    "indir_deg_test = CONST.DEGRADED_TEST_PATH  # Modifica in base alla posizione delle immagini\n",
    "outdir = '../dataset/features'  # Modifica in base alla posizione delle feature\n",
    "# Carica le immagini dal CSV\n",
    "image_names, labels = load_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGBMeanFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: rgb_mean\n",
      "Caricamento delle feature da ../dataset/features/train_small_rgb_mean_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 3 con rgb_mean\n",
      "Caricamento delle feature da ../dataset/features/test_info_rgb_mean_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 3 con rgb_mean\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_rgb_mean_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 3 con rgb_mean\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Pipeline' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 35\u001b[0m\n\u001b[1;32m     23\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     24\u001b[0m     KNN(\u001b[38;5;241m3\u001b[39m, standardize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     25\u001b[0m     KNN(\u001b[38;5;241m11\u001b[39m, standardize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     KNN(\u001b[38;5;241m51\u001b[39m, standardize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     32\u001b[0m ]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[0;32m---> 35\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     36\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/mnt/c/Users/Telemaco/Desktop/elab/vipm_project/code/vipm_pipeline.py:98\u001b[0m, in \u001b[0;36mKNN.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mfit(X, y)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Pipeline' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(RGBMeanFeatureExtractor())\n",
    "# Itera sugli estrattori di feature\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "models = [\n",
    "    KNN(3, standardize=False),\n",
    "    KNN(11, standardize=False),\n",
    "    KNN(21, standardize=False),\n",
    "    KNN(51, standardize=False),\n",
    "    KNN(3, standardize=True),\n",
    "    KNN(11, standardize=True),\n",
    "    KNN(21, standardize=True),\n",
    "    KNN(51, standardize=True),\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "        print(f\"Model: KNN - neighbours: {model.n_neighbors} - standardize: {model.standardize}\")\n",
    "        print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LBPFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: lbp\n",
      "Caricamento delle feature da ../dataset/features/train_small_lbp_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 10 con lbp\n",
      "Caricamento delle feature da ../dataset/features/test_info_lbp_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 10 con lbp\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_lbp_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 10 con lbp\n",
      "Model: KNN - neighbours: 3 - standardize: False\n",
      "acc: 0.006920126730031682 - prec: 0.006664945346145457 - rec: 0.006920126730031682 - fscore: 0.005689952374190868\n",
      "Model: KNN - neighbours: 11 - standardize: False\n",
      "acc: 0.007170251792562948 - prec: 0.0051934902814881004 - rec: 0.007170251792562948 - fscore: 0.004661735734523695\n",
      "Model: KNN - neighbours: 21 - standardize: False\n",
      "acc: 0.007170251792562948 - prec: 0.007361320354385149 - rec: 0.007170251792562948 - fscore: 0.0062722888777011696\n",
      "Model: KNN - neighbours: 51 - standardize: False\n",
      "acc: 0.007170251792562948 - prec: 0.006898903400223636 - rec: 0.007170251792562948 - fscore: 0.005830223075290567\n",
      "Model: KNN - neighbours: 3 - standardize: True\n",
      "acc: 0.007337001834250459 - prec: 0.006205889129163987 - rec: 0.007337001834250459 - fscore: 0.005835474507207108\n",
      "Model: KNN - neighbours: 11 - standardize: True\n",
      "acc: 0.007587126896781724 - prec: 0.0070913416844386595 - rec: 0.007587126896781724 - fscore: 0.005305544143477088\n",
      "Model: KNN - neighbours: 21 - standardize: True\n",
      "acc: 0.006836751709187927 - prec: 0.0066201746584399835 - rec: 0.006836751709187927 - fscore: 0.00571245916200369\n",
      "Model: KNN - neighbours: 51 - standardize: True\n",
      "acc: 0.008254127063531767 - prec: 0.00767157059698821 - rec: 0.008254127063531767 - fscore: 0.006470361101822689\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(LBPFeatureExtractor())\n",
    "# Itera sugli estrattori di feature\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "models = [\n",
    "    KNN(3, standardize=False),\n",
    "    KNN(11, standardize=False),\n",
    "    KNN(21, standardize=False),\n",
    "    KNN(51, standardize=False),\n",
    "    KNN(3, standardize=True),\n",
    "    KNN(11, standardize=True),\n",
    "    KNN(21, standardize=True),\n",
    "    KNN(51, standardize=True),\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "        print(f\"Model: KNN - neighbours: {model.n_neighbors} - standardize: {model.standardize}\")\n",
    "        print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: lab\n",
      "Caricamento delle feature da ../dataset/features/train_small_lab_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 768 con lab\n",
      "Caricamento delle feature da ../dataset/features/test_info_lab_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 768 con lab\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_lab_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 768 con lab\n",
      "Model: KNN - neighbours: 3 - standardize: False\n",
      "acc: 0.013089878272469569 - prec: 0.012835338168677185 - rec: 0.013089878272469569 - fscore: 0.010687142902499391\n",
      "Model: KNN - neighbours: 11 - standardize: False\n",
      "acc: 0.01183925295981324 - prec: 0.014981169225351037 - rec: 0.01183925295981324 - fscore: 0.009537432916112594\n",
      "Model: KNN - neighbours: 21 - standardize: False\n",
      "acc: 0.012339503084875771 - prec: 0.01243860204256451 - rec: 0.012339503084875771 - fscore: 0.01023208458196908\n",
      "Model: KNN - neighbours: 51 - standardize: False\n",
      "acc: 0.01567450391862598 - prec: 0.0161897184637832 - rec: 0.01567450391862598 - fscore: 0.012077996445953223\n",
      "Model: KNN - neighbours: 3 - standardize: True\n",
      "acc: 0.01584125396031349 - prec: 0.015739552552225058 - rec: 0.01584125396031349 - fscore: 0.012977854389417922\n",
      "Model: KNN - neighbours: 11 - standardize: True\n",
      "acc: 0.016508254127063533 - prec: 0.02258780340165186 - rec: 0.016508254127063533 - fscore: 0.014024193901607463\n",
      "Model: KNN - neighbours: 21 - standardize: True\n",
      "acc: 0.016841754210438552 - prec: 0.021508860299339096 - rec: 0.016841754210438552 - fscore: 0.015082729398595483\n",
      "Model: KNN - neighbours: 51 - standardize: True\n",
      "acc: 0.019593129898282474 - prec: 0.022423882614728337 - rec: 0.019593129898282474 - fscore: 0.015773267236277973\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(LABFeatureExtractor())\n",
    "# Itera sugli estrattori di feature\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "models = [\n",
    "    KNN(3, standardize=False),\n",
    "    KNN(11, standardize=False),\n",
    "    KNN(21, standardize=False),\n",
    "    KNN(51, standardize=False),\n",
    "    KNN(3, standardize=True),\n",
    "    KNN(11, standardize=True),\n",
    "    KNN(21, standardize=True),\n",
    "    KNN(51, standardize=True),\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "        print(f\"Model: KNN - neighbours: {model.n_neighbors} - standardize: {model.standardize}\")\n",
    "        print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGBMeanFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train Loss: 5.6571, Train Accuracy: 0.45%\n",
      "  Val Loss: 5.5269, Val Accuracy: 0.20%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.5737, Train Accuracy: 0.52%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.5293, Train Accuracy: 0.92%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.4760, Train Accuracy: 0.80%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.4331, Train Accuracy: 1.00%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.3864, Train Accuracy: 1.34%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.3907, Train Accuracy: 1.27%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.3766, Train Accuracy: 1.05%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.3733, Train Accuracy: 1.32%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.3716, Train Accuracy: 1.15%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.3558, Train Accuracy: 1.25%\n",
      "  Val Loss: 5.5301, Val Accuracy: 0.20%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n",
      "acc: 0.00016675004168751042 - prec: 2.7805576402786462e-08 - rec: 0.00016675004168751042 - fscore: 5.560188118956666e-08\n"
     ]
    }
   ],
   "source": [
    "one_layer_model = OneLayerNetwork(3, 251)\n",
    "one_layer_optimizer = torch.optim.Adam(one_layer_model.parameters(), lr=0.01)\n",
    "one_layer_scheduler = torch.optim.lr_scheduler.StepLR(one_layer_optimizer, step_size=5, gamma=0.1)\n",
    "one_layer_model_option = ModelOptions(torch.nn.CrossEntropyLoss(), one_layer_optimizer, one_layer_scheduler, input_dim = 3)\n",
    "nn = NeuralNetwork(one_layer_model, one_layer_model_option)\n",
    "\n",
    "training_set = FeatureDataset(f\"{outdir}/train_small_rgb_mean_features_normalized.npz\",\n",
    "                              type='test',\n",
    "                              target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set = FeatureDataset(f\"{outdir}/test_info_rgb_mean_features_normalized.npz\",\n",
    "                          type='test',\n",
    "                          target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set_degraded = FeatureDataset(f'{outdir}/test_deg_info_rgb_mean_features_normalized.npz',\n",
    "                                type='test',\n",
    "                                target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # Remaining 20% for validation\n",
    "training_set, val_dataset = random_split(training_set, [train_size, val_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "test_degraded_loader = DataLoader(test_set_degraded, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "\n",
    "nn.fit(train_loader, val_loader)\n",
    "mean_loss, accuracy, y_pred, y_test = nn.predict(test_loader)\n",
    "\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBPFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5020, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f\"{outdir}/train_small_lbp_features_normalized.npz\")['X'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train Loss: 5.6357, Train Accuracy: 0.40%\n",
      "  Val Loss: 5.5263, Val Accuracy: 0.20%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.5654, Train Accuracy: 0.47%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.5205, Train Accuracy: 0.65%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.4836, Train Accuracy: 0.72%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.4561, Train Accuracy: 0.62%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.4403, Train Accuracy: 1.02%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.4359, Train Accuracy: 1.02%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.4315, Train Accuracy: 0.57%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.4203, Train Accuracy: 0.82%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.4205, Train Accuracy: 1.12%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.4169, Train Accuracy: 0.97%\n",
      "  Val Loss: 5.5309, Val Accuracy: 0.30%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n",
      "acc: 0.004418876104719026 - prec: 0.0002506086171170238 - rec: 0.004418876104719026 - fscore: 0.00030669239856054905\n"
     ]
    }
   ],
   "source": [
    "one_layer_model = OneLayerNetwork(10, 251)\n",
    "one_layer_optimizer = torch.optim.Adam(one_layer_model.parameters(), lr=0.01)\n",
    "one_layer_scheduler = torch.optim.lr_scheduler.StepLR(one_layer_optimizer, step_size=5, gamma=0.1)\n",
    "one_layer_model_option = ModelOptions(torch.nn.CrossEntropyLoss(), one_layer_optimizer, one_layer_scheduler, input_dim = 10)\n",
    "nn = NeuralNetwork(one_layer_model, one_layer_model_option)\n",
    "\n",
    "training_set = FeatureDataset(f\"{outdir}/train_small_lbp_features_normalized.npz\",\n",
    "                              type='test',\n",
    "                              target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set = FeatureDataset(f\"{outdir}/test_info_lbp_features_normalized.npz\",\n",
    "                          type='test',\n",
    "                          target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set_degraded = FeatureDataset(f'{outdir}/test_deg_info_lbp_features_normalized.npz',\n",
    "                                type='test',\n",
    "                                target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # Remaining 20% for validation\n",
    "training_set, val_dataset = random_split(training_set, [train_size, val_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "test_degraded_loader = DataLoader(test_set_degraded, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "\n",
    "nn.fit(train_loader, val_loader)\n",
    "mean_loss, accuracy, y_pred, y_test = nn.predict(test_loader)\n",
    "\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LABFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11994, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(f\"{outdir}/test_deg_info_lab_features_normalized.npz\")['X'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train Loss: 5.6679, Train Accuracy: 0.17%\n",
      "  Val Loss: 5.5260, Val Accuracy: 0.50%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.6004, Train Accuracy: 0.47%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.5641, Train Accuracy: 0.62%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.5233, Train Accuracy: 0.57%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.4773, Train Accuracy: 0.80%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.4507, Train Accuracy: 1.05%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.4212, Train Accuracy: 1.00%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.4277, Train Accuracy: 0.97%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.4248, Train Accuracy: 1.32%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.4013, Train Accuracy: 1.37%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.4091, Train Accuracy: 1.02%\n",
      "  Val Loss: 5.5305, Val Accuracy: 0.40%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n",
      "acc: 0.004502251125562781 - prec: 0.00010996444664907475 - rec: 0.004502251125562781 - fscore: 0.00020507433211673436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "one_layer_model = OneLayerNetwork(768, 251)\n",
    "one_layer_optimizer = torch.optim.Adam(one_layer_model.parameters(), lr=0.01)\n",
    "one_layer_scheduler = torch.optim.lr_scheduler.StepLR(one_layer_optimizer, step_size=5, gamma=0.1)\n",
    "one_layer_model_option = ModelOptions(torch.nn.CrossEntropyLoss(), one_layer_optimizer, one_layer_scheduler, input_dim = 768)\n",
    "nn = NeuralNetwork(one_layer_model, one_layer_model_option)\n",
    "\n",
    "training_set = FeatureDataset(f\"{outdir}/train_small_lab_features_normalized.npz\",\n",
    "                              type='test',\n",
    "                              target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set = FeatureDataset(f\"{outdir}/test_info_lab_features_normalized.npz\",\n",
    "                          type='test',\n",
    "                          target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set_degraded = FeatureDataset(f'{outdir}/test_deg_info_lab_features_normalized.npz',\n",
    "                                type='test',\n",
    "                                target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # Remaining 20% for validation\n",
    "training_set, val_dataset = random_split(training_set, [train_size, val_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "test_degraded_loader = DataLoader(test_set_degraded, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "\n",
    "nn.fit(train_loader, val_loader)\n",
    "mean_loss, accuracy, y_pred, y_test = nn.predict(test_loader)\n",
    "\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_1.8.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
