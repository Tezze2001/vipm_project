{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handcrafted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "\n",
    "project_path = os.path.abspath(\"../code\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "project_path = os.path.abspath(\"../networks\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "from models import *\n",
    "from vipm_features import *\n",
    "import vipm_costants as CONST\n",
    "from vipm_pipeline import *\n",
    "from dataset import *\n",
    "\n",
    "def load_csv(csv_path):\n",
    "    data = pd.read_csv(csv_path, header=None, names=['image_name', 'label'])\n",
    "    return data['image_name'].tolist(), data['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = CONST.SMALL_TRAINING_NAME_PATH\n",
    "csv_unlabeled = CONST.TRAINING_NAME_PATH\n",
    "csv_test = CONST.TESTING_NAME_PATH\n",
    "csv_test_deg = CONST.DEG_TESTING_NAME_PATH\n",
    "indir_train = CONST.SMALL_TRAINING_PATH  # Modifica in base alla posizione delle immagini\n",
    "indir_test = CONST.TEST_PATH  # Modifica in base alla posizione delle immagini\n",
    "indir_deg_test = CONST.DEGRADED_TEST_PATH  # Modifica in base alla posizione delle immagini\n",
    "outdir = '../dataset/features'  # Modifica in base alla posizione delle feature\n",
    "# Carica le immagini dal CSV\n",
    "image_names, labels = load_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGBMeanFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: rgb_mean\n",
      "Caricamento delle feature da ../dataset/features/train_small_rgb_mean_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 3 con rgb_mean\n",
      "Caricamento delle feature da ../dataset/features/test_info_rgb_mean_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 3 con rgb_mean\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_rgb_mean_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 3 con rgb_mean\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(RGBMeanFeatureExtractor())\n",
    "# Itera sugli estrattori di feature\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "models = [\n",
    "    KNN(3, standardize=False),\n",
    "    KNN(11, standardize=False),\n",
    "    KNN(21, standardize=False),\n",
    "    KNN(51, standardize=False),\n",
    "    KNN(3, standardize=True),\n",
    "    KNN(11, standardize=True),\n",
    "    KNN(21, standardize=True),\n",
    "    KNN(51, standardize=True),\n",
    "]\n",
    "\n",
    "with open('./classification_hand_crafted_rgb_knn.log', 'w') as f:\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            acc1 = top_k_accuracy_score(y_test, y_pred, k=1)\n",
    "            acc5 = top_k_accuracy_score(y_test, y_pred, k=5)\n",
    "            acc10 = top_k_accuracy_score(y_test, y_pred, k=10)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Model: KNN - neighbours: {model.n_neighbors} - standardize: {model.standardize}\", file=f)\n",
    "            print(f\"Test set\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "        \n",
    "        y_pred = model.predict_proba(X_test_deg)\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            acc1 = top_k_accuracy_score(y_test_deg, y_pred, k=1)\n",
    "            acc5 = top_k_accuracy_score(y_test_deg, y_pred, k=5)\n",
    "            acc10 = top_k_accuracy_score(y_test_deg, y_pred, k=10)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test_deg, y_pred, average='weighted')\n",
    "            print(f\"Test set degrated\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "            print(file=f)\n",
    "            print(file=f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LBPFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: lbp\n",
      "Caricamento delle feature da ../dataset/features/train_small_lbp_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 10 con lbp\n",
      "Caricamento delle feature da ../dataset/features/test_info_lbp_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 10 con lbp\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_lbp_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 10 con lbp\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(LBPFeatureExtractor())\n",
    "# Itera sugli estrattori di feature\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "models = [\n",
    "    KNN(3, standardize=False),\n",
    "    KNN(11, standardize=False),\n",
    "    KNN(21, standardize=False),\n",
    "    KNN(51, standardize=False),\n",
    "    KNN(3, standardize=True),\n",
    "    KNN(11, standardize=True),\n",
    "    KNN(21, standardize=True),\n",
    "    KNN(51, standardize=True),\n",
    "]\n",
    "\n",
    "\n",
    "with open('./classification_hand_crafted_lbp_knn.log', 'w') as f:\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            acc1 = top_k_accuracy_score(y_test, y_pred, k=1)\n",
    "            acc5 = top_k_accuracy_score(y_test, y_pred, k=5)\n",
    "            acc10 = top_k_accuracy_score(y_test, y_pred, k=10)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Model: KNN - neighbours: {model.n_neighbors} - standardize: {model.standardize}\", file=f)\n",
    "            print(f\"Test set\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "        \n",
    "        y_pred = model.predict_proba(X_test_deg)\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            acc1 = top_k_accuracy_score(y_test_deg, y_pred, k=1)\n",
    "            acc5 = top_k_accuracy_score(y_test_deg, y_pred, k=5)\n",
    "            acc10 = top_k_accuracy_score(y_test_deg, y_pred, k=10)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test_deg, y_pred, average='weighted')\n",
    "            print(f\"Test set degrated\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "            print(file=f)\n",
    "            print(file=f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LABFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: lab\n",
      "Caricamento delle feature da ../dataset/features/train_small_lab_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 768 con lab\n",
      "Caricamento delle feature da ../dataset/features/test_info_lab_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 768 con lab\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_lab_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 768 con lab\n"
     ]
    }
   ],
   "source": [
    "extractor = FeatureExtractor(LABFeatureExtractor())\n",
    "# Itera sugli estrattori di feature\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "models = [\n",
    "    KNN(3, standardize=False),\n",
    "    KNN(11, standardize=False),\n",
    "    KNN(21, standardize=False),\n",
    "    KNN(51, standardize=False),\n",
    "    KNN(3, standardize=True),\n",
    "    KNN(11, standardize=True),\n",
    "    KNN(21, standardize=True),\n",
    "    KNN(51, standardize=True),\n",
    "]\n",
    "\n",
    "with open('./classification_hand_crafted_lab_knn.log', 'w') as f:\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict_proba(X_test)\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            acc1 = top_k_accuracy_score(y_test, y_pred, k=1)\n",
    "            acc5 = top_k_accuracy_score(y_test, y_pred, k=5)\n",
    "            acc10 = top_k_accuracy_score(y_test, y_pred, k=10)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Model: KNN - neighbours: {model.n_neighbors} - standardize: {model.standardize}\", file=f)\n",
    "            print(f\"Test set\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "        \n",
    "        y_pred = model.predict_proba(X_test_deg)\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            acc1 = top_k_accuracy_score(y_test_deg, y_pred, k=1)\n",
    "            acc5 = top_k_accuracy_score(y_test_deg, y_pred, k=5)\n",
    "            acc10 = top_k_accuracy_score(y_test_deg, y_pred, k=10)\n",
    "            y_pred = np.argmax(y_pred, axis=1)\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test_deg, y_pred, average='weighted')\n",
    "            print(f\"Test set degrated\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "            print(file=f)\n",
    "            print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RGBMeanFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train Loss: 5.6461, Train Accuracy: 0.30%\n",
      "  Val Loss: 5.5279, Val Accuracy: 0.30%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.5738, Train Accuracy: 0.57%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.5241, Train Accuracy: 0.55%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.4681, Train Accuracy: 0.77%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.4418, Train Accuracy: 0.85%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.3989, Train Accuracy: 1.34%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.3953, Train Accuracy: 1.22%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.3802, Train Accuracy: 1.39%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.3796, Train Accuracy: 1.17%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.3688, Train Accuracy: 1.29%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.3652, Train Accuracy: 1.34%\n",
      "  Val Loss: 5.5338, Val Accuracy: 0.20%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n",
      "Epoch 1/100:\n",
      "  Train Loss: 5.8397, Train Accuracy: 0.32%\n",
      "  Val Loss: 5.5426, Val Accuracy: 0.30%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.6987, Train Accuracy: 1.12%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.4743, Train Accuracy: 1.05%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.4164, Train Accuracy: 1.20%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.3317, Train Accuracy: 1.37%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.2719, Train Accuracy: 1.67%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.2517, Train Accuracy: 1.84%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.2328, Train Accuracy: 1.82%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.2029, Train Accuracy: 1.89%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.1846, Train Accuracy: 2.02%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.1782, Train Accuracy: 1.99%\n",
      "  Val Loss: 5.6181, Val Accuracy: 0.80%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "n_features = np.load(f'{outdir}/train_small_rgb_mean_features_normalized.npz')['X'].shape[1]\n",
    "n_classes = 251\n",
    "\n",
    "models = []\n",
    "\n",
    "model = OneLayerNetwork(n_features, n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "model_option = ModelOptions(torch.nn.CrossEntropyLoss(), optimizer, scheduler, input_dim = n_features)\n",
    "models.append(('OneLayerNetwork', NeuralNetwork(model, model_option)))\n",
    "\n",
    "model = ClassifierNetwork(n_features, n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "model_option = ModelOptions(torch.nn.CrossEntropyLoss(), optimizer, scheduler, input_dim = n_features)\n",
    "models.append(('ClassifierNetwork', NeuralNetwork(model, model_option)))\n",
    "\n",
    "training_set = FeatureDataset(f\"{outdir}/train_small_rgb_mean_features_normalized.npz\",\n",
    "                              type='test',\n",
    "                              target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set = FeatureDataset(f\"{outdir}/test_info_rgb_mean_features_normalized.npz\",\n",
    "                          type='test',\n",
    "                          target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set_degraded = FeatureDataset(f'{outdir}/test_deg_info_rgb_mean_features_normalized.npz',\n",
    "                                type='test',\n",
    "                                target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # Remaining 20% for validation\n",
    "training_set, val_dataset = random_split(training_set, [train_size, val_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "test_degraded_loader = DataLoader(test_set_degraded, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "\n",
    "with open(f'./classification_hand_crafted_rgb_nn.log', 'w') as f:\n",
    "    for name, model in models:\n",
    "        model.fit(train_loader, val_loader)\n",
    "\n",
    "        mean_loss, acc1, acc5, acc10, y_pred, y_test = model.predict(test_loader)\n",
    "\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Model: {name}\", file=f)\n",
    "            print(f\"Test set\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "            \n",
    "        mean_loss, acc1, acc5, acc10, y_pred, y_test = model.predict(test_degraded_loader)\n",
    "\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Test set degrated\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "        print(file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBPFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train Loss: 5.6377, Train Accuracy: 0.40%\n",
      "  Val Loss: 5.5250, Val Accuracy: 0.40%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.5641, Train Accuracy: 0.42%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.5207, Train Accuracy: 0.60%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.4776, Train Accuracy: 0.60%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.4518, Train Accuracy: 0.77%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.4390, Train Accuracy: 0.95%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.4338, Train Accuracy: 0.97%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.4308, Train Accuracy: 0.85%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.4282, Train Accuracy: 0.70%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.4211, Train Accuracy: 1.00%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.4136, Train Accuracy: 1.15%\n",
      "  Val Loss: 5.5302, Val Accuracy: 0.30%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n",
      "Epoch 1/100:\n",
      "  Train Loss: 5.7816, Train Accuracy: 0.47%\n",
      "  Val Loss: 5.5291, Val Accuracy: 0.70%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.4613, Train Accuracy: 1.54%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.2605, Train Accuracy: 2.22%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.1256, Train Accuracy: 2.91%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 4.9914, Train Accuracy: 3.51%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 4.8978, Train Accuracy: 4.41%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 4.8707, Train Accuracy: 4.51%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 4.8490, Train Accuracy: 4.96%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 4.8348, Train Accuracy: 5.23%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 4.8008, Train Accuracy: 4.86%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 4.7865, Train Accuracy: 5.20%\n",
      "  Val Loss: 5.5569, Val Accuracy: 0.50%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "n_features = np.load(f'{outdir}/train_small_lbp_features_normalized.npz')['X'].shape[1]\n",
    "n_classes = 251\n",
    "\n",
    "models = []\n",
    "\n",
    "model = OneLayerNetwork(n_features, n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "model_option = ModelOptions(torch.nn.CrossEntropyLoss(), optimizer, scheduler, input_dim = n_features)\n",
    "models.append(('OneLayerNetwork', NeuralNetwork(model, model_option)))\n",
    "\n",
    "model = ClassifierNetwork(n_features, n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "model_option = ModelOptions(torch.nn.CrossEntropyLoss(), optimizer, scheduler, input_dim = n_features)\n",
    "models.append(('ClassifierNetwork', NeuralNetwork(model, model_option)))\n",
    "\n",
    "training_set = FeatureDataset(f\"{outdir}/train_small_lbp_features_normalized.npz\",\n",
    "                              type='test',\n",
    "                              target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set = FeatureDataset(f\"{outdir}/test_info_lbp_features_normalized.npz\",\n",
    "                          type='test',\n",
    "                          target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set_degraded = FeatureDataset(f'{outdir}/test_deg_info_lbp_features_normalized.npz',\n",
    "                                type='test',\n",
    "                                target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # Remaining 20% for validation\n",
    "training_set, val_dataset = random_split(training_set, [train_size, val_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "test_degraded_loader = DataLoader(test_set_degraded, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "\n",
    "with open(f'./classification_hand_crafted_lbp_nn.log', 'w') as f:\n",
    "    for name, model in models:\n",
    "\n",
    "        model.fit(train_loader, val_loader)\n",
    "\n",
    "        mean_loss, acc1, acc5, acc10, y_pred, y_test = model.predict(test_loader)\n",
    "\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Model: {name}\", file=f)\n",
    "            print(f\"Test set\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "            \n",
    "        mean_loss, acc1, acc5, acc10, y_pred, y_test = model.predict(test_degraded_loader)\n",
    "\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Test set degrated\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "        print(file=f)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LABFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train Loss: 5.6667, Train Accuracy: 0.37%\n",
      "  Val Loss: 5.5263, Val Accuracy: 0.50%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.6115, Train Accuracy: 0.27%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.5676, Train Accuracy: 0.47%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.5203, Train Accuracy: 0.80%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.4842, Train Accuracy: 0.85%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.4539, Train Accuracy: 0.90%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.4273, Train Accuracy: 1.32%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.4366, Train Accuracy: 1.12%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.4180, Train Accuracy: 0.97%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.4209, Train Accuracy: 0.85%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.4068, Train Accuracy: 1.02%\n",
      "  Val Loss: 5.5296, Val Accuracy: 0.20%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n",
      "Epoch 1/100:\n",
      "  Train Loss: 5.7060, Train Accuracy: 0.35%\n",
      "  Val Loss: 5.5251, Val Accuracy: 0.10%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.0379, Train Accuracy: 5.10%\n",
      "  Val Loss: 5.5205, Val Accuracy: 0.50%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 4.6133, Train Accuracy: 8.91%\n",
      "  Val Loss: 5.4974, Val Accuracy: 0.70%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 4.2780, Train Accuracy: 12.38%\n",
      "  Val Loss: 5.4797, Val Accuracy: 1.29%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 3.9945, Train Accuracy: 16.31%\n",
      "  Val Loss: 5.4685, Val Accuracy: 1.10%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 3.7496, Train Accuracy: 19.97%\n",
      "  Val Loss: 5.4574, Val Accuracy: 0.90%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 3.6937, Train Accuracy: 22.09%\n",
      "  Val Loss: 5.4479, Val Accuracy: 1.29%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 3.6450, Train Accuracy: 23.23%\n",
      "  Val Loss: 5.4372, Val Accuracy: 1.69%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 3.6050, Train Accuracy: 24.03%\n",
      "  Val Loss: 5.4257, Val Accuracy: 1.69%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 3.5605, Train Accuracy: 24.45%\n",
      "  Val Loss: 5.4136, Val Accuracy: 1.59%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 3.5169, Train Accuracy: 25.70%\n",
      "  Val Loss: 5.4013, Val Accuracy: 1.49%\n",
      "Epoch 12/100:\n",
      "  Train Loss: 3.5086, Train Accuracy: 25.82%\n",
      "  Val Loss: 5.3903, Val Accuracy: 1.49%\n",
      "Epoch 13/100:\n",
      "  Train Loss: 3.5200, Train Accuracy: 25.97%\n",
      "  Val Loss: 5.3795, Val Accuracy: 1.39%\n",
      "Epoch 14/100:\n",
      "  Train Loss: 3.5126, Train Accuracy: 26.17%\n",
      "  Val Loss: 5.3690, Val Accuracy: 1.49%\n",
      "Epoch 15/100:\n",
      "  Train Loss: 3.5037, Train Accuracy: 26.05%\n",
      "  Val Loss: 5.3590, Val Accuracy: 1.49%\n",
      "Epoch 16/100:\n",
      "  Train Loss: 3.4960, Train Accuracy: 26.97%\n",
      "  Val Loss: 5.3500, Val Accuracy: 1.49%\n",
      "Epoch 17/100:\n",
      "  Train Loss: 3.5055, Train Accuracy: 26.62%\n",
      "  Val Loss: 5.3423, Val Accuracy: 1.59%\n",
      "Epoch 18/100:\n",
      "  Train Loss: 3.4942, Train Accuracy: 26.12%\n",
      "  Val Loss: 5.3364, Val Accuracy: 1.69%\n",
      "Epoch 19/100:\n",
      "  Train Loss: 3.4974, Train Accuracy: 25.82%\n",
      "  Val Loss: 5.3325, Val Accuracy: 1.69%\n",
      "Epoch 20/100:\n",
      "  Train Loss: 3.5116, Train Accuracy: 26.57%\n",
      "  Val Loss: 5.3311, Val Accuracy: 1.69%\n",
      "Epoch 21/100:\n",
      "  Train Loss: 3.4919, Train Accuracy: 26.67%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 22/100:\n",
      "  Train Loss: 3.5043, Train Accuracy: 26.62%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 23/100:\n",
      "  Train Loss: 3.4962, Train Accuracy: 25.82%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 24/100:\n",
      "  Train Loss: 3.5058, Train Accuracy: 26.05%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 25/100:\n",
      "  Train Loss: 3.5005, Train Accuracy: 26.27%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 26/100:\n",
      "  Train Loss: 3.4961, Train Accuracy: 26.34%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 27/100:\n",
      "  Train Loss: 3.4965, Train Accuracy: 26.25%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 28/100:\n",
      "  Train Loss: 3.4916, Train Accuracy: 26.25%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 29/100:\n",
      "  Train Loss: 3.4968, Train Accuracy: 26.49%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "Epoch 30/100:\n",
      "  Train Loss: 3.5006, Train Accuracy: 26.52%\n",
      "  Val Loss: 5.3326, Val Accuracy: 1.69%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "n_features = np.load(f'{outdir}/train_small_lab_features_normalized.npz')['X'].shape[1]\n",
    "n_classes = 251\n",
    "\n",
    "models = []\n",
    "\n",
    "model = OneLayerNetwork(n_features, n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "model_option = ModelOptions(torch.nn.CrossEntropyLoss(), optimizer, scheduler, input_dim = n_features)\n",
    "models.append(('OneLayerNetwork', NeuralNetwork(model, model_option)))\n",
    "\n",
    "model = ClassifierNetwork(n_features, n_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "model_option = ModelOptions(torch.nn.CrossEntropyLoss(), optimizer, scheduler, input_dim = n_features)\n",
    "models.append(('ClassifierNetwork', NeuralNetwork(model, model_option)))\n",
    "\n",
    "training_set = FeatureDataset(f\"{outdir}/train_small_lab_features_normalized.npz\",\n",
    "                              type='test',\n",
    "                              target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set = FeatureDataset(f\"{outdir}/test_info_lab_features_normalized.npz\",\n",
    "                          type='test',\n",
    "                          target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set_degraded = FeatureDataset(f'{outdir}/test_deg_info_lab_features_normalized.npz',\n",
    "                                type='test',\n",
    "                                target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # Remaining 20% for validation\n",
    "training_set, val_dataset = random_split(training_set, [train_size, val_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "test_degraded_loader = DataLoader(test_set_degraded, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "\n",
    "with open(f'./classification_hand_crafted_lab_nn.log', 'w') as f:\n",
    "    for name, model in models:\n",
    "\n",
    "        model.fit(train_loader, val_loader)\n",
    "\n",
    "        mean_loss, acc1, acc5, acc10, y_pred, y_test = model.predict(test_loader)\n",
    "\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Model: {name}\", file=f)\n",
    "            print(f\"Test set\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "            \n",
    "        mean_loss, acc1, acc5, acc10, y_pred, y_test = model.predict(test_degraded_loader)\n",
    "\n",
    "        with warnings.catch_warnings(action=\"ignore\"):\n",
    "            precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            print(f\"Test set degrated\", file=f)\n",
    "            print(f\"acc@1: {acc1} - acc@5: {acc5} - acc@10: {acc10} - prec: {precision} - rec: {recall} - fscore: {fscore}\", file=f)\n",
    "\n",
    "        print(file=f)\n",
    "        print(file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_1.8.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
