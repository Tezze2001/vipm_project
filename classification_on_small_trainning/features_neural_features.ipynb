{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "\n",
    "project_path = os.path.abspath(\"../code\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "project_path = os.path.abspath(\"../networks\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "from models import *\n",
    "from vipm_features import *\n",
    "import vipm_costants as CONST\n",
    "from vipm_pipeline import *\n",
    "from dataset import *\n",
    "\n",
    "def load_csv(csv_path):\n",
    "    data = pd.read_csv(csv_path, header=None, names=['image_name', 'label'])\n",
    "    return data['image_name'].tolist(), data['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = CONST.SMALL_TRAINING_NAME_PATH\n",
    "csv_unlabeled = CONST.TRAINING_NAME_PATH\n",
    "csv_test = CONST.TESTING_NAME_PATH\n",
    "csv_test_deg = CONST.DEG_TESTING_NAME_PATH\n",
    "indir_train = CONST.SMALL_TRAINING_PATH  # Modifica in base alla posizione delle immagini\n",
    "indir_test = CONST.TEST_PATH  # Modifica in base alla posizione delle immagini\n",
    "indir_deg_test = CONST.DEGRADED_TEST_PATH  # Modifica in base alla posizione delle immagini\n",
    "outdir = '../dataset/features'  # Modifica in base alla posizione delle feature\n",
    "# Carica le immagini dal CSV\n",
    "image_names, labels = load_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: resnet50\n",
      "Caricamento delle feature da ../dataset/features/train_small_resnet50_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 2048 con resnet50\n",
      "Caricamento delle feature da ../dataset/features/test_info_resnet50_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 2048 con resnet50\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_resnet50_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 2048 con resnet50\n",
      "<class 'sklearn.pipeline.Pipeline'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[247 234 222 ...  72 124 109].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     35\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m---> 36\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(y_test_deg)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     38\u001b[0m         acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_test_deg, y_pred)\n",
      "File \u001b[0;32m/mnt/c/Users/Telemaco/Desktop/elab/vipm_project/code/vipm_pipeline.py:102\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/pipeline.py:601\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    600\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m--> 601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n\u001b[1;32m    604\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/neighbors/_classification.py:271\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_[np\u001b[38;5;241m.\u001b[39margmax(probabilities, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkneighbors(X, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    272\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/neighbors/_base.py:825\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    823\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    827\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/utils/validation.py:1050\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1044\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1045\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1046\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1047\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1048\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m             )\n\u001b[0;32m-> 1050\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array\u001b[38;5;241m.\u001b[39mdtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not compatible with arrays of bytes/strings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConvert your data to numeric values explicitly instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[247 234 222 ...  72 124 109].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "\n",
    "extractor = FeatureExtractor(ResNet50FeatureExtractor())\n",
    "\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "models = [\n",
    "    KNN(3, standardize=False),\n",
    "    KNN(11, standardize=False),\n",
    "    KNN(21, standardize=False),\n",
    "    KNN(51, standardize=False),\n",
    "    KNN(3, standardize=True),\n",
    "    KNN(11, standardize=True),\n",
    "    KNN(21, standardize=True),\n",
    "    KNN(51, standardize=True),\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test_deg)\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        acc = accuracy_score(y_test_deg, y_pred)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_test_deg, y_pred, average='weighted')\n",
    "        print(f\"Model: KNN - neighbours: {model.n_neighbors} - standardize: {model.standardize}\")\n",
    "        print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: resnet50\n",
      "Caricamento delle feature da ../dataset/features/train_small_resnet50_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 2048 con resnet50\n",
      "Caricamento delle feature da ../dataset/features/test_info_resnet50_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 2048 con resnet50\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_resnet50_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 2048 con resnet50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extractor = FeatureExtractor(ResNet50FeatureExtractor())\n",
    "\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train Loss: 5.6562, Train Accuracy: 0.35%\n",
      "  Val Loss: 5.5278, Val Accuracy: 0.30%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.5823, Train Accuracy: 0.50%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.5325, Train Accuracy: 0.75%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.4835, Train Accuracy: 0.85%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.4339, Train Accuracy: 0.97%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.3906, Train Accuracy: 1.12%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.3982, Train Accuracy: 1.44%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.3770, Train Accuracy: 1.47%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.3794, Train Accuracy: 1.32%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.3655, Train Accuracy: 1.12%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.3537, Train Accuracy: 1.62%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n",
      "acc: 0.00016675004168751042 - prec: 2.7828778652788787e-08 - rec: 0.00016675004168751042 - fscore: 5.564827021108307e-08\n"
     ]
    }
   ],
   "source": [
    "one_layer_model = OneLayerNetwork(3, 251)\n",
    "one_layer_optimizer = torch.optim.Adam(one_layer_model.parameters(), lr=0.01)\n",
    "one_layer_scheduler = torch.optim.lr_scheduler.StepLR(one_layer_optimizer, step_size=5, gamma=0.1)\n",
    "one_layer_model_option = ModelOptions(torch.nn.CrossEntropyLoss(), one_layer_optimizer, one_layer_scheduler, input_dim = 3)\n",
    "nn = NeuralNetwork(one_layer_model, one_layer_model_option)\n",
    "\n",
    "training_set = FeatureDataset(f\"{outdir}/train_small_rgb_mean_features_normalized.npz\",\n",
    "                              type='test',\n",
    "                              target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set = FeatureDataset(f\"{outdir}/test_info_rgb_mean_features_normalized.npz\",\n",
    "                          type='test',\n",
    "                          target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set_degraded = FeatureDataset(f'{outdir}/test_deg_info_rgb_mean_features_normalized.npz',\n",
    "                                type='test',\n",
    "                                target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # Remaining 20% for validation\n",
    "training_set, val_dataset = random_split(training_set, [train_size, val_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "test_degraded_loader = DataLoader(test_set_degraded, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "\n",
    "nn.fit(train_loader, val_loader)\n",
    "mean_loss, accuracy, y_pred, y_test = nn.predict(test_loader)\n",
    "\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_1.8.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
