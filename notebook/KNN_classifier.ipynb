{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.models import resnet50\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shutil\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using KNN small training + 10% unlabeled training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_knn(n_neighbors, dataset, validation, standardize = True, print_conf_matrix=True, print_class_report = True, weights='uniform'):\n",
    "    std_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),              # Standardization step\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)) \n",
    "    ])\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('knn', KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights))  \n",
    "    ])\n",
    "\n",
    "    if standardize:\n",
    "        std_pipeline.fit(dataset[0], dataset[1])\n",
    "        predictions = std_pipeline.predict(validation[0])\n",
    "    else:\n",
    "        pipeline.fit(dataset[0], dataset[1])\n",
    "        predictions = pipeline.predict(validation[0])\n",
    "\n",
    "    if print_conf_matrix:\n",
    "        cm = confusion_matrix(validation[1], predictions, labels=range(251))\n",
    "\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Set the figure size\n",
    "        plt.figure(figsize=(20, 15))\n",
    "\n",
    "        # Plot using Seaborn's heatmap\n",
    "        sns.heatmap(\n",
    "            cm_normalized,\n",
    "            cmap='viridis',\n",
    "            cbar=True,\n",
    "            square=False,  # Use rectangular cells for better fit\n",
    "            xticklabels=25,  # Show only 25 x-axis labels to avoid clutter\n",
    "            yticklabels=25  # Show only 25 y-axis labels to avoid clutter\n",
    "        )\n",
    "\n",
    "        # Rotate and set axis labels\n",
    "        plt.xlabel('Predicted Labels', fontsize=14)\n",
    "        plt.ylabel('True Labels', fontsize=14)\n",
    "        plt.title('Normalized Confusion Matrix (250 Classes)', fontsize=18)\n",
    "        plt.xticks(rotation=45, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "    \n",
    "    if print_class_report:\n",
    "        cr = classification_report(validation[1], predictions, labels=range(251), zero_division=0)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(cr)\n",
    "    \n",
    "    if standardize:\n",
    "        std_pipeline.fit(np.concatenate([dataset[0], validation[0]]), \n",
    "                         np.concatenate([dataset[1], validation[1]]))\n",
    "        return std_pipeline\n",
    "    else:\n",
    "        pipeline.fit(np.concatenate([dataset[0], validation[0]]), \n",
    "                     np.concatenate([dataset[1], validation[1]]))\n",
    "        return pipeline\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load('./dataset/features_extended_10.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = fit_knn(9, (dataset['X_train'], dataset['y_train']), (dataset['X_val'], dataset['y_val']), standardize=False, print_conf_matrix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = np.load(\"./dataset/test_features_resnet50.npz\")\n",
    "X_test = test_dataset['X']\n",
    "y_test = test_dataset['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr = classification_report(y_test, y_pred, labels=range(251), zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def fit_svm(dataset, validation, print_conf_matrix=True, print_class_report = True):\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),      # Step 1: Standardize the data\n",
    "        ('svc', SVC())                     # Step 2: Apply SVC model\n",
    "    ])\n",
    "\n",
    "    # 4. Set up the parameter grid for GridSearchCV\n",
    "    param_grid = {\n",
    "        'scaler': [StandardScaler(), None],          # Include 'None' for no scaling\n",
    "        'svc__kernel': ['linear', 'rbf'],             # Testing linear and RBF kernels\n",
    "        'svc__C': [0.1, 1, 10],                       # Regularization parameter\n",
    "        'svc__gamma': ['scale', 'auto']          # Kernel coefficient\n",
    "    }\n",
    "\n",
    "    X_grid = np.concatenate([dataset[0], validation[0]])\n",
    "    y_grid = np.concatenate([dataset[1], validation[1]])\n",
    "\n",
    "    predefined_split = PredefinedSplit(test_fold=np.concatenate([np.zeros(len(dataset[0])), np.ones(len(validation[0]))]))\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=predefined_split, verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_grid, y_grid)\n",
    "\n",
    "\n",
    "    # Best parameters found by GridSearchCV\n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "    # Use the best model from grid search\n",
    "    best_svc_model = grid_search.best_estimator_\n",
    "\n",
    "    best_svc_model.fit(dataset[0], dataset[1])\n",
    "    predictions = best_svc_model.predict(validation[0])\n",
    "\n",
    "    if print_conf_matrix:\n",
    "        cm = confusion_matrix(validation[1], predictions, labels=range(251))\n",
    "\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "        # Set the figure size\n",
    "        plt.figure(figsize=(20, 15))\n",
    "\n",
    "        # Plot using Seaborn's heatmap\n",
    "        sns.heatmap(\n",
    "            cm_normalized,\n",
    "            cmap='viridis',\n",
    "            cbar=True,\n",
    "            square=False,  # Use rectangular cells for better fit\n",
    "            xticklabels=25,  # Show only 25 x-axis labels to avoid clutter\n",
    "            yticklabels=25  # Show only 25 y-axis labels to avoid clutter\n",
    "        )\n",
    "\n",
    "        # Rotate and set axis labels\n",
    "        plt.xlabel('Predicted Labels', fontsize=14)\n",
    "        plt.ylabel('True Labels', fontsize=14)\n",
    "        plt.title('Normalized Confusion Matrix (250 Classes)', fontsize=18)\n",
    "        plt.xticks(rotation=45, fontsize=10)\n",
    "        plt.yticks(fontsize=10)\n",
    "\n",
    "        # Display the plot\n",
    "        plt.show()\n",
    "    \n",
    "    if print_class_report:\n",
    "        cr = classification_report(validation[1], predictions, labels=range(251), zero_division=0)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(cr)\n",
    "    \n",
    "    best_svc_model.fit(np.concatenate([dataset[0], validation[0]]), \n",
    "                        np.concatenate([dataset[1], validation[1]]))\n",
    "    return best_svc_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "svm = fit_svm((dataset['X_train'], dataset['y_train']), (dataset['X_val'], dataset['y_val']), print_conf_matrix=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      2\u001b[0m cr \u001b[38;5;241m=\u001b[39m classification_report(y_test, y_pred, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m251\u001b[39m), zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mClassification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/pipeline.py:601\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    600\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mtransform(Xt)\n\u001b[0;32m--> 601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n\u001b[1;32m    604\u001b[0m routed_params \u001b[38;5;241m=\u001b[39m process_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/svm/_base.py:813\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    811\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 813\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/svm/_base.py:430\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    428\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[1;32m    429\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss_1.8.0/lib/python3.12/site-packages/sklearn/svm/_base.py:449\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    442\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be equal to \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe number of samples at training time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    445\u001b[0m         )\n\u001b[1;32m    447\u001b[0m svm_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n\u001b[0;32m--> 449\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m libsvm\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    450\u001b[0m     X,\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dual_coef_,\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intercept_,\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    458\u001b[0m     svm_type\u001b[38;5;241m=\u001b[39msvm_type,\n\u001b[1;32m    459\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel,\n\u001b[1;32m    460\u001b[0m     degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegree,\n\u001b[1;32m    461\u001b[0m     coef0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef0,\n\u001b[1;32m    462\u001b[0m     gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gamma,\n\u001b[1;32m    463\u001b[0m     cache_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_size,\n\u001b[1;32m    464\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_pred = svm.predict(X_test)\n",
    "cr = classification_report(y_test, y_pred, labels=range(251), zero_division=0)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_1.8.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
