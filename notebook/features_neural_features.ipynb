{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "import warnings\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F \n",
    "\n",
    "project_path = os.path.abspath(\"../code\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "project_path = os.path.abspath(\"../networks\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "from models import *\n",
    "from vipm_features import *\n",
    "import vipm_costants as CONST\n",
    "from vipm_pipeline import *\n",
    "from dataset import *\n",
    "\n",
    "def load_csv(csv_path):\n",
    "    data = pd.read_csv(csv_path, header=None, names=['image_name', 'label'])\n",
    "    return data['image_name'].tolist(), data['label'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = CONST.SMALL_TRAINING_NAME_PATH\n",
    "csv_unlabeled = CONST.TRAINING_NAME_PATH\n",
    "csv_test = CONST.TESTING_NAME_PATH\n",
    "csv_test_deg = CONST.DEG_TESTING_NAME_PATH\n",
    "indir_train = CONST.SMALL_TRAINING_PATH  # Modifica in base alla posizione delle immagini\n",
    "indir_test = CONST.TEST_PATH  # Modifica in base alla posizione delle immagini\n",
    "indir_deg_test = CONST.DEGRADED_TEST_PATH  # Modifica in base alla posizione delle immagini\n",
    "outdir = '../dataset/features'  # Modifica in base alla posizione delle feature\n",
    "# Carica le immagini dal CSV\n",
    "image_names, labels = load_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: resnet50\n",
      "Caricamento delle feature da ../dataset/features/train_small_resnet50_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 2048 con resnet50\n",
      "Caricamento delle feature da ../dataset/features/test_info_resnet50_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 2048 con resnet50\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_resnet50_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 2048 con resnet50\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Model: KNN - neighbours: 3 - standardize: False\n",
      "acc: 0.10630315157578789 - prec: 0.1543341099830251 - rec: 0.10630315157578789 - fscore: 0.10553754918220129\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Model: KNN - neighbours: 11 - standardize: False\n",
      "acc: 0.1424045356011339 - prec: 0.17383900388408238 - rec: 0.1424045356011339 - fscore: 0.14069705390083145\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Model: KNN - neighbours: 21 - standardize: False\n",
      "acc: 0.15482741370685343 - prec: 0.17453738775754338 - rec: 0.15482741370685343 - fscore: 0.14870549177539172\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Model: KNN - neighbours: 51 - standardize: False\n",
      "acc: 0.16649991662497915 - prec: 0.18938081712157478 - rec: 0.16649991662497915 - fscore: 0.15587060453050924\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Model: KNN - neighbours: 3 - standardize: True\n",
      "acc: 0.10196765049191263 - prec: 0.1688319232304718 - rec: 0.10196765049191263 - fscore: 0.10509651613771644\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Model: KNN - neighbours: 11 - standardize: True\n",
      "acc: 0.13248290812072702 - prec: 0.18396290275066265 - rec: 0.13248290812072702 - fscore: 0.1361812689711426\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Model: KNN - neighbours: 21 - standardize: True\n",
      "acc: 0.1418209104552276 - prec: 0.1780633238784606 - rec: 0.1418209104552276 - fscore: 0.1392078179444713\n",
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "Model: KNN - neighbours: 51 - standardize: True\n",
      "acc: 0.14682341170585292 - prec: 0.1922113612771981 - rec: 0.14682341170585292 - fscore: 0.14076100307934758\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extractor = FeatureExtractor(ResNet50FeatureExtractor())\n",
    "\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "models = [\n",
    "    KNN(3, standardize=False),\n",
    "    KNN(11, standardize=False),\n",
    "    KNN(21, standardize=False),\n",
    "    KNN(51, standardize=False),\n",
    "    KNN(3, standardize=True),\n",
    "    KNN(11, standardize=True),\n",
    "    KNN(21, standardize=True),\n",
    "    KNN(51, standardize=True),\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test_deg)\n",
    "    with warnings.catch_warnings(action=\"ignore\"):\n",
    "        acc = accuracy_score(y_test_deg, y_pred)\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_test_deg, y_pred, average='weighted')\n",
    "        print(f\"Model: KNN - neighbours: {model.n_neighbors} - standardize: {model.standardize}\")\n",
    "        print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50FeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing extractor: resnet50\n",
      "Caricamento delle feature da ../dataset/features/train_small_resnet50_features_normalized.npz\n",
      "Estratte 5020 feature di dimensione 2048 con resnet50\n",
      "Caricamento delle feature da ../dataset/features/test_info_resnet50_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 2048 con resnet50\n",
      "Caricamento delle feature da ../dataset/features/test_deg_info_resnet50_features_normalized.npz\n",
      "Estratte 11994 feature di dimensione 2048 con resnet50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extractor = FeatureExtractor(ResNet50FeatureExtractor())\n",
    "\n",
    "print(f\"\\nTesting extractor: {extractor.name}\")\n",
    "\n",
    "try:\n",
    "    X_train, y_train, _ = extractor.transform(**{\"csv\":csv_path, \"indir\":indir_train, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_train.shape[0]} feature di dimensione {X_train.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test, y_test, _ = extractor.transform(**{\"csv\":csv_test, \"indir\":indir_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test.shape[0]} feature di dimensione {X_test.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")\n",
    "\n",
    "try:\n",
    "    X_test_deg, y_test_deg, _ = extractor.transform(**{\"csv\":csv_test_deg, \"indir\":indir_deg_test, \"outdir\":outdir, \"normalize\":True})\n",
    "    print(f\"Estratte {X_test_deg.shape[0]} feature di dimensione {X_test_deg.shape[1]} con {extractor.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante l'elaborazione con {extractor.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:\n",
      "  Train Loss: 5.6562, Train Accuracy: 0.35%\n",
      "  Val Loss: 5.5278, Val Accuracy: 0.30%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.5823, Train Accuracy: 0.50%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.5325, Train Accuracy: 0.75%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.4835, Train Accuracy: 0.85%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.4339, Train Accuracy: 0.97%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.3906, Train Accuracy: 1.12%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.3982, Train Accuracy: 1.44%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.3770, Train Accuracy: 1.47%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.3794, Train Accuracy: 1.32%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.3655, Train Accuracy: 1.12%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.3537, Train Accuracy: 1.62%\n",
      "  Val Loss: 5.5344, Val Accuracy: 0.20%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n",
      "acc: 0.00016675004168751042 - prec: 2.7828778652788787e-08 - rec: 0.00016675004168751042 - fscore: 5.564827021108307e-08\n"
     ]
    }
   ],
   "source": [
    "one_layer_model = OneLayerNetwork(3, 251)\n",
    "one_layer_optimizer = torch.optim.Adam(one_layer_model.parameters(), lr=0.01)\n",
    "one_layer_scheduler = torch.optim.lr_scheduler.StepLR(one_layer_optimizer, step_size=5, gamma=0.1)\n",
    "one_layer_model_option = ModelOptions(torch.nn.CrossEntropyLoss(), one_layer_optimizer, one_layer_scheduler, input_dim = 3)\n",
    "nn = NeuralNetwork(one_layer_model, one_layer_model_option)\n",
    "\n",
    "training_set = FeatureDataset(f\"{outdir}/train_small_rgb_mean_features_normalized.npz\",\n",
    "                              type='test',\n",
    "                              target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set = FeatureDataset(f\"{outdir}/test_info_rgb_mean_features_normalized.npz\",\n",
    "                          type='test',\n",
    "                          target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "test_set_degraded = FeatureDataset(f'{outdir}/test_deg_info_rgb_mean_features_normalized.npz',\n",
    "                                type='test',\n",
    "                                target_transform=lambda y: F.one_hot(y, num_classes=one_layer_model_option.num_classes))\n",
    "\n",
    "train_size = int(0.8 * len(training_set))  # 80% for training\n",
    "val_size = len(training_set) - train_size  # Remaining 20% for validation\n",
    "training_set, val_dataset = random_split(training_set, [train_size, val_size], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "test_degraded_loader = DataLoader(test_set_degraded, batch_size=one_layer_model_option.batch_size, shuffle=False)\n",
    "\n",
    "nn.fit(train_loader, val_loader)\n",
    "mean_loss, top1_accuracy, top5_accuracy, top10_accuracy, y_pred_top1, y_test = nn.predict(test_loader)\n",
    "\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    print(f\"acc: {acc} - prec: {precision} - rec: {recall} - fscore: {fscore}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss_1.8.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
