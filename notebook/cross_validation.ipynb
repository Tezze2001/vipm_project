{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, PredefinedSplit, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Add project path\n",
    "project_path = os.path.abspath(\"../code\")\n",
    "sys.path.append(project_path)\n",
    "from vipm_features import ResNet50FeatureExtractor\n",
    "\n",
    "project_path = os.path.abspath(\"../code\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "project_path = os.path.abspath(\"../networks\")  # Adatta il percorso a dove si trova il tuo progetto\n",
    "sys.path.append(project_path)\n",
    "from models import *\n",
    "from vipm_features import *\n",
    "import vipm_costants as CONST\n",
    "from vipm_pipeline import *\n",
    "from dataset import *\n",
    "import torch\n",
    "\n",
    "# Configure environment variable\n",
    "LOKY_MAX_CPU_COUNT = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPES 20, (5020, 2048), (1, 5020)\n",
      "Caricamento delle feature da ../features\\train_small_resnet50_features_normalized.npz\n",
      "Caricamento delle feature da ../features\\val_info_resnet50_features_normalized.npz\n",
      "Caricamento delle feature da ../features\\features_test_degraded_normalized.npz\n"
     ]
    }
   ],
   "source": [
    "# Carica il file CSV\n",
    "def load_csv(csv_path):\n",
    "    data = pd.read_csv(csv_path, header=None, names=['image_name', 'label'])\n",
    "    return data['image_name'].tolist(), data['label'].tolist()\n",
    "\n",
    "# Carica il file compresso NPZ\n",
    "def load_features(npz_path):\n",
    "    data = np.load(npz_path)\n",
    "    features = data['features']\n",
    "    labels = data['labels']\n",
    "    return features, labels\n",
    "\n",
    "# Percorsi\n",
    "csv_path = '../dataset/train_small.csv'   \n",
    "csv_unlabeled = '../dataset/train_unlabeled.csv'\n",
    "csv_test = '../dataset/val_info.csv'\n",
    "\n",
    "indir = '../dataset/train_set'  # Modifica in base alla posizione delle immagini\n",
    "test_dir = '../dataset/val_set'\n",
    "test_degraded_dir = '../dataset/val_set_degraded'\n",
    "\n",
    "outdir = '../features'  # Modifica in base alla posizione delle feature\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "# Carica le features recuperate\n",
    "\n",
    "# 20 \n",
    "npz_path = os.path.join(outdir, 'features_unlabeled_retrived.npz')\n",
    "features_20, labels_20 = load_features(npz_path)\n",
    "\n",
    "# 20 cleaned\n",
    "npz_path = os.path.join(outdir, 'features_unlabeled_retrived_cleaned.npz')\n",
    "features_20_cleaned, labels_20_cleaned = load_features(npz_path)\n",
    "\n",
    "print(\"SHAPES 20, {}, {}\".format(features_20.shape, labels_20.shape))\n",
    "\n",
    "# 40\n",
    "npz_path = os.path.join(outdir, 'features_unlabeled_retrived_40.npz')\n",
    "features_40, labels_40 = load_features(npz_path)\n",
    "\n",
    "# 40 cleaned\n",
    "npz_path = os.path.join(outdir, 'features_unlabeled_retrived_cleaned_40.npz')\n",
    "features_40_cleaned, labels_40_cleaned = load_features(npz_path)\n",
    "\n",
    "# 80\n",
    "npz_path = os.path.join(outdir, 'features_unlabeled_retrived_80.npz')\n",
    "features_80, labels_80 = load_features(npz_path)\n",
    "\n",
    "# 80 cleaned\n",
    "npz_path = os.path.join(outdir, 'features_unlabeled_retrived_cleaned_80.npz')\n",
    "features_80_cleaned, labels_80_cleaned = load_features(npz_path)\n",
    "\n",
    "# feature small \n",
    "npz_path = os.path.join(outdir, 'features_small_filtered.npz')\n",
    "features_small_filtered, labels_small_filtered = load_features(npz_path)\n",
    "\n",
    "# Carica le immagini dal CSV\n",
    "image_names, labels_small = load_csv(csv_path)\n",
    "labels_small = np.array(labels_small)\n",
    "extractor = ResNet50FeatureExtractor()\n",
    "features_small, _, _ = extractor.get_features(csv=csv_path, indir=indir, outdir=outdir, normalize=True)\n",
    "\n",
    "# Test set \n",
    "image_names_test, labels_test = load_csv(csv_test)\n",
    "labels_test = np.array(labels_test)\n",
    "features_test, _, _ = extractor.get_features(csv=csv_test, indir=test_dir, outdir=outdir, normalize=True)\n",
    "\n",
    "# Test set degraded\n",
    "image_names_test_degraded, labels_test_degraded = load_csv(csv_test)\n",
    "labels_test_degraded = np.array(labels_test_degraded)\n",
    "features_test_degraded, _, _ = extractor.get_features(csv=csv_test, indir=test_degraded_dir, outdir=outdir, normalize=True, file_name='features_test_degraded_normalized.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Cleaned: False\n",
      "  Valutazione per K=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sguid\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\sguid\\miniconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    KNN: Accuracy=0.14003984063745017, Var Accuracy=0.0002200758718115585, Top-5 Accuracy=0.3264940239043824, Var Top-5 Accuracy=0.0001061094268344945, Top-10 Accuracy=0.33944223107569715, Var Top-10 Accuracy=9.8966683068523e-05\n",
      "Epoch 1/100:\n",
      "  Train Loss: 5.6604, Train Accuracy: 0.35%\n",
      "  Val Loss: 5.5261, Val Accuracy: 0.40%\n",
      "Epoch 2/100:\n",
      "  Train Loss: 5.5854, Train Accuracy: 0.30%\n",
      "  Val Loss: 5.5256, Val Accuracy: 0.40%\n",
      "Epoch 3/100:\n",
      "  Train Loss: 5.4951, Train Accuracy: 0.80%\n",
      "  Val Loss: 5.5166, Val Accuracy: 1.10%\n",
      "Epoch 4/100:\n",
      "  Train Loss: 5.3603, Train Accuracy: 1.44%\n",
      "  Val Loss: 5.4638, Val Accuracy: 1.20%\n",
      "Epoch 5/100:\n",
      "  Train Loss: 5.2159, Train Accuracy: 1.79%\n",
      "  Val Loss: 5.3382, Val Accuracy: 1.39%\n",
      "Epoch 6/100:\n",
      "  Train Loss: 5.1106, Train Accuracy: 2.51%\n",
      "  Val Loss: 5.1964, Val Accuracy: 1.69%\n",
      "Epoch 7/100:\n",
      "  Train Loss: 5.0935, Train Accuracy: 2.42%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 8/100:\n",
      "  Train Loss: 5.0558, Train Accuracy: 1.94%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 9/100:\n",
      "  Train Loss: 5.0420, Train Accuracy: 3.01%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 10/100:\n",
      "  Train Loss: 5.0329, Train Accuracy: 2.59%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 11/100:\n",
      "  Train Loss: 5.0061, Train Accuracy: 2.71%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 12/100:\n",
      "  Train Loss: 5.0014, Train Accuracy: 2.69%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 13/100:\n",
      "  Train Loss: 5.0176, Train Accuracy: 2.61%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 14/100:\n",
      "  Train Loss: 4.9987, Train Accuracy: 2.84%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 15/100:\n",
      "  Train Loss: 5.0041, Train Accuracy: 2.74%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "Epoch 16/100:\n",
      "  Train Loss: 5.0026, Train Accuracy: 2.59%\n",
      "  Val Loss: 5.2583, Val Accuracy: 1.10%\n",
      "\n",
      "Early stopping triggered. Stopping training.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (1004) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 116\u001b[0m\n\u001b[0;32m    113\u001b[0m logs\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m, small_cleaned, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKNN\u001b[39m\u001b[38;5;124m\"\u001b[39m, k, mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    KNN: Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Var Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Top-5 Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_top_5_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Var Top-5 Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_top_5_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Top-10 Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_top_10_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Var Top-10 Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_top_10_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 116\u001b[0m mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, mean_loss, var_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_neural_network_one_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmall_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmall_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m logs\u001b[38;5;241m.\u001b[39mappend([\u001b[38;5;241m0\u001b[39m, small_cleaned, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m\"\u001b[39m, k, mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, mean_loss, var_loss])\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    NN: Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Var Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Top-5 Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_top_5_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Var Top-5 Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_top_5_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Top-10 Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_top_10_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Var Top-10 Accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_top_10_acc_cv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Var Loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 74\u001b[0m, in \u001b[0;36mtrain_and_evaluate_neural_network_one_layer\u001b[1;34m(X, y, cv_splits, epochs, batch_size)\u001b[0m\n\u001b[0;32m     71\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_fold, batch_size\u001b[38;5;241m=\u001b[39mone_layer_model_option\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     73\u001b[0m nn\u001b[38;5;241m.\u001b[39mfit(train_loader, val_loader)\n\u001b[1;32m---> 74\u001b[0m loss, top1_accuracy, top5_accuracy, top10_accuracy, y_pred_top1, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     77\u001b[0m accuracies\u001b[38;5;241m.\u001b[39mappend(top1_accuracy)\n",
      "File \u001b[1;32mc:\\Users\\sguid\\Uni\\visual\\project\\vipm_project\\code\\vipm_pipeline.py:259\u001b[0m, in \u001b[0;36mNeuralNetwork.predict\u001b[1;34m(self, test_loader)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_loader):\n\u001b[1;32m--> 259\u001b[0m     mean_loss, top1_accuracy, top5_accuracy, top10_accuracy, y_pred_top1, y_test_hot_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__test_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m     y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test_hot_encoded, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_loss, top1_accuracy, top5_accuracy, top10_accuracy, y_pred_top1, y_test\n",
      "File \u001b[1;32mc:\\Users\\sguid\\Uni\\visual\\project\\vipm_project\\code\\vipm_pipeline.py:197\u001b[0m, in \u001b[0;36mNeuralNetwork.__test_evaluate\u001b[1;34m(self, test_loader)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# Check if true labels are in the top-5 predictions\u001b[39;00m\n\u001b[0;32m    196\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m y_batch\n\u001b[1;32m--> 197\u001b[0m correct_top5 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mpredicted_top5\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Check if true labels are in the top-10 predictions\u001b[39;00m\n\u001b[0;32m    200\u001b[0m correct_top10 \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted_top10 \u001b[38;5;241m==\u001b[39m true_labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (1004) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "log_path = \"results_log.csv\"\n",
    "log_columns = [\n",
    "    \"Dimension\", \"Small Cleaned\", \"Dim Cleaned\", \"Model\", \"K\", \"Accuracy\", \"Var Accuracy\", \"Top-5 Accuracy\", \"Var Top-5 Accuracy\", \"Top-10 Accuracy\", \"Var Top-10 Accuracy\", \"Loss\", \"Var Loss\"\n",
    "]\n",
    "logs = []\n",
    "\n",
    "# Combinazioni di dimensioni e feature\n",
    "configurations = [\n",
    "    {\"dimension\": 20, \"features\": features_20, \"labels\": labels_20, \"cleaned_features\": features_20_cleaned, \"cleaned_labels\": labels_20_cleaned},\n",
    "    {\"dimension\": 40, \"features\": features_40, \"labels\": labels_40, \"cleaned_features\": features_40_cleaned, \"cleaned_labels\": labels_40_cleaned},\n",
    "    {\"dimension\": 80, \"features\": features_80, \"labels\": labels_80, \"cleaned_features\": features_80_cleaned, \"cleaned_labels\": labels_80_cleaned}\n",
    "]\n",
    "\n",
    "def train_and_evaluate_knn_with_variance(X, y, k, cv_splits=5):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    # Cross-validation per accuracy\n",
    "    accuracies = cross_val_score(knn, X, y, cv=skf, scoring='accuracy')\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    var_accuracy = np.var(accuracies)\n",
    "\n",
    "    # Cross-validation per top-5 accuracy\n",
    "    top_5_accuracies = []\n",
    "    top_10_accuracies = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        knn.fit(X_train, y_train)\n",
    "        top_5_predictions = np.argsort(knn.predict_proba(X_val), axis=1)[:, -5:]\n",
    "        top_5_accuracy = np.mean([y in top_5 for y, top_5 in zip(y_val, top_5_predictions)])\n",
    "        top_5_accuracies.append(top_5_accuracy)\n",
    "        top_10_predictions = np.argsort(knn.predict_proba(X_val), axis=1)[:, -10:]\n",
    "        top_10_accuracy = np.mean([y in top_10 for y, top_10 in zip(y_val, top_10_predictions)])\n",
    "        top_10_accuracies.append(top_10_accuracy)\n",
    "\n",
    "    mean_top_5_accuracy = np.mean(top_5_accuracies)\n",
    "    var_top_5_accuracy = np.var(top_5_accuracies)\n",
    "    \n",
    "    mean_top_10_accuracy = np.mean(top_10_accuracies)\n",
    "    var_top_10_accuracy = np.var(top_10_accuracies)\n",
    "\n",
    "    return mean_accuracy, var_accuracy, mean_top_5_accuracy, var_top_5_accuracy, mean_top_10_accuracy, var_top_10_accuracy\n",
    "\n",
    "def train_and_evaluate_neural_network_one_layer(X, y, cv_splits=5, epochs=10, batch_size=32):\n",
    "    one_layer_model = OneLayerNetwork(2048, 251)\n",
    "    one_layer_optimizer = torch.optim.Adam(one_layer_model.parameters(), lr=0.01)\n",
    "    one_layer_scheduler = torch.optim.lr_scheduler.StepLR(one_layer_optimizer, step_size=5, gamma=0.1)\n",
    "    one_layer_model_option = ModelOptions(torch.nn.CrossEntropyLoss(), one_layer_optimizer, one_layer_scheduler, input_dim = 2048)\n",
    "    nn = NeuralNetwork(one_layer_model, one_layer_model_option)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    accuracies = []\n",
    "    top_5_accuracies = []\n",
    "    top_10_accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X, y):\n",
    "        # Split data into training and validation sets for this fold\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Convert to torch tensors and create datasets\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.LongTensor(y_train)\n",
    "        X_val_tensor = torch.FloatTensor(X_val)\n",
    "        y_val_tensor = torch.LongTensor(y_val)\n",
    "        \n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset,  batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=one_layer_model_option.batch_size, shuffle=True)\n",
    "\n",
    "        nn.fit(train_loader, val_loader)\n",
    "        loss, top1_accuracy, top5_accuracy, top10_accuracy, y_pred_top1, y_test = nn.predict(val_loader)\n",
    "\n",
    "        losses.append(loss)\n",
    "        accuracies.append(top1_accuracy)\n",
    "        top_5_accuracies.append(top5_accuracy)\n",
    "        top_10_accuracies.append(top10_accuracy)\n",
    "\n",
    "\n",
    "    mean_loss = np.mean(losses)\n",
    "    var_loss = np.var(losses)\n",
    "    \n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    var_accuracy = np.var(accuracies)\n",
    "    \n",
    "    mean_top_5_accuracy = np.mean(top_5_accuracies)\n",
    "    var_top_5_accuracy = np.var(top_5_accuracies)\n",
    "    \n",
    "    mean_top_10_accuracy = np.mean(top_10_accuracies)\n",
    "    var_top_10_accuracy = np.var(top_10_accuracies)\n",
    "\n",
    "    return mean_accuracy, var_accuracy, mean_top_5_accuracy, var_top_5_accuracy, mean_top_10_accuracy, var_top_10_accuracy, mean_loss, var_loss\n",
    "\n",
    "# Test small come baseline\n",
    "for small_cleaned in [False, True]:\n",
    "    print(f\"Small Cleaned: {small_cleaned}\")\n",
    "\n",
    "    if small_cleaned:\n",
    "        small_features = features_small_filtered\n",
    "        small_labels = labels_small_filtered\n",
    "    else:\n",
    "        small_features = features_small\n",
    "        small_labels = labels_small\n",
    "\n",
    "    # Prova con diversi valori di K\n",
    "    for k in [5, 10, 25, 50, 100, 150, 200]:\n",
    "        print(f\"  Valutazione per K={k}\")\n",
    "\n",
    "        # Cross-validation 80-20 con funzione dedicata\n",
    "        mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv = train_and_evaluate_knn_with_variance(small_features, small_labels, k)\n",
    "        logs.append([0, small_cleaned, False, \"KNN\", k, mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, 0, 0])\n",
    "        print(f\"    KNN: Accuracy={mean_acc_cv}, Var Accuracy={var_acc_cv}, Top-5 Accuracy={mean_top_5_acc_cv}, Var Top-5 Accuracy={var_top_5_acc_cv}, Top-10 Accuracy={mean_top_10_acc_cv}, Var Top-10 Accuracy={var_top_10_acc_cv}\")\n",
    "\n",
    "        mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, mean_loss, var_loss = train_and_evaluate_neural_network_one_layer(small_features, small_labels)\n",
    "        logs.append([0, small_cleaned, False, \"NN\", k, mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, mean_loss, var_loss])\n",
    "        print(f\"    NN: Accuracy={mean_acc_cv}, Var Accuracy={var_acc_cv}, Top-5 Accuracy={mean_top_5_acc_cv}, Var Top-5 Accuracy={var_top_5_acc_cv}, Top-10 Accuracy={mean_top_10_acc_cv}, Var Top-10 Accuracy={var_top_10_acc_cv}, Loss={mean_loss}, Var Loss={var_loss}\")\n",
    "        \n",
    "        # # Test su test set\n",
    "        # knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        # knn.fit(small_features, small_labels)\n",
    "        # top_5_predictions_test = np.argsort(knn.predict_proba(features_test), axis=1)[:, -5:]\n",
    "        # acc_test = knn.score(features_test, labels_test)\n",
    "        # top_5_acc_test = np.mean([y in top_5 for y, top_5 in zip(labels_test, top_5_predictions_test)])\n",
    "        # logs.append([0, small_cleaned, False, \"Test Set (only on small)\", k, acc_test, top_5_acc_test])\n",
    "        # print(f\"    Test Set (only on small): Accuracy={acc_test}, Top-5 Accuracy={top_5_acc_test}\")\n",
    "\n",
    "        # # Test su test set degraded\n",
    "        # top_5_predictions_test_degraded = np.argsort(knn.predict_proba(features_test_degraded), axis=1)[:, -5:]\n",
    "        # acc_test_degraded = knn.score(features_test_degraded, labels_test_degraded)\n",
    "        # top_5_acc_test_degraded = np.mean([y in top_5 for y, top_5 in zip(labels_test_degraded, top_5_predictions_test_degraded)])\n",
    "        # logs.append([0, small_cleaned, False, \"Test Set Degraded (only on small)\", k, acc_test_degraded, top_5_acc_test_degraded])\n",
    "        # print(f\"    Test Set Degraded (only on small): Accuracy={acc_test_degraded}, Top-5 Accuracy={top_5_acc_test_degraded}\")\n",
    "        \n",
    "        \n",
    "# Cross-validation 80-20 e test\n",
    "for config in configurations:\n",
    "    dim = config[\"dimension\"]\n",
    "    print(f\"Iniziando configurazione per dimensione: {dim}\")\n",
    "\n",
    "    # Combinazioni di feature tra small e dimensione specifica\n",
    "    for small_cleaned in [False, True]:\n",
    "        for dim_cleaned in [False, True]:\n",
    "\n",
    "            if small_cleaned:\n",
    "                small_features = features_small_filtered\n",
    "                small_labels = labels_small_filtered\n",
    "            else:\n",
    "                small_features = features_small\n",
    "                small_labels = labels_small\n",
    "\n",
    "            if dim_cleaned:\n",
    "                current_features = config[\"cleaned_features\"]\n",
    "                current_labels = config[\"cleaned_labels\"][0]\n",
    "            else:\n",
    "                current_features = config[\"features\"]\n",
    "                current_labels = config[\"labels\"][0]\n",
    "\n",
    "            print(f\"  Small Cleaned: {small_cleaned}, Dim Cleaned: {dim_cleaned}\")\n",
    "\n",
    "            # Unione delle feature\n",
    "            combined_features = np.concatenate((current_features, small_features), axis=0)\n",
    "            combined_labels = np.concatenate((current_labels, small_labels), axis=0)\n",
    "\n",
    "            # Prova con diversi valori di K\n",
    "            for k in [5, 10, 25, 50, 100, 150]:\n",
    "                print(f\"    Valutazione per K={k}\")\n",
    "\n",
    "                # Cross-validation 80-20 con funzione dedicata\n",
    "                mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv = train_and_evaluate_knn_with_variance(combined_features, combined_labels, k)\n",
    "                logs.append([dim, small_cleaned, dim_cleaned, \"KNN\", k, mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, 0, 0])\n",
    "                print(f\"      KNN: Accuracy={mean_acc_cv}, Var Accuracy={var_acc_cv}, Top-5 Accuracy={mean_top_5_acc_cv}, Var Top-5 Accuracy={var_top_5_acc_cv}, Top-10 Accuracy={mean_top_10_acc_cv}, Var Top-10 Accuracy={var_top_10_acc_cv}\")\n",
    "                \n",
    "                mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, mean_loss, var_loss = train_and_evaluate_neural_network_one_layer(combined_features, combined_labels)\n",
    "                logs.append([dim, small_cleaned, dim_cleaned, \"NN\", k, mean_acc_cv, var_acc_cv, mean_top_5_acc_cv, var_top_5_acc_cv, mean_top_10_acc_cv, var_top_10_acc_cv, mean_loss, var_loss])\n",
    "                print(f\"      NN: Accuracy={mean_acc_cv}, Var Accuracy={var_acc_cv}, Top-5 Accuracy={mean_top_5_acc_cv}, Var Top-5 Accuracy={var_top_5_acc_cv}, Top-10 Accuracy={mean_top_10_acc_cv}, Var Top-10 Accuracy={var_top_10_acc_cv}, Loss={mean_loss}, Var Loss={var_loss}\")\n",
    "\n",
    "                # # Validation set solo con feature della dimensione specifica\n",
    "                # knn = KNeighborsClassifier(n_neighbors=k)\n",
    "                # knn.fit(current_features, current_labels)\n",
    "                # top_5_predictions_val = np.argsort(knn.predict_proba(small_features), axis=1)[:, -5:]\n",
    "                # acc_val = knn.score(small_features, small_labels)\n",
    "                # top_5_acc_val = np.mean([y in top_5 for y, top_5 in zip(small_labels, top_5_predictions_val)])\n",
    "                # logs.append([dim, small_cleaned, dim_cleaned, \"Validation small\", k, acc_val, top_5_acc_val])\n",
    "                # print(f\"      Validation small: Accuracy={acc_val}, Top-5 Accuracy={top_5_acc_val}\")\n",
    "\n",
    "                # # Test su test set\n",
    "                # top_5_predictions_test = np.argsort(knn.predict_proba(features_test), axis=1)[:, -5:]\n",
    "                # acc_test = knn.score(features_test, labels_test)\n",
    "                # top_5_acc_test = np.mean([y in top_5 for y, top_5 in zip(labels_test, top_5_predictions_test)])\n",
    "                # logs.append([dim, small_cleaned, dim_cleaned, \"Test Set (only on retrived)\", k, acc_test, top_5_acc_test])\n",
    "                # print(f\"      Test Set (only on retrived): Accuracy={acc_test}, Top-5 Accuracy={top_5_acc_test}\")\n",
    "\n",
    "                # # Test su test set degraded\n",
    "                # top_5_predictions_test_degraded = np.argsort(knn.predict_proba(features_test_degraded), axis=1)[:, -5:]\n",
    "                # acc_test_degraded = knn.score(features_test_degraded, labels_test_degraded)\n",
    "                # top_5_acc_test_degraded = np.mean([y in top_5 for y, top_5 in zip(labels_test_degraded, top_5_predictions_test_degraded)])\n",
    "                # logs.append([dim, small_cleaned, dim_cleaned, \"Test Set Degraded (only on retrived)\", k, acc_test_degraded, top_5_acc_test_degraded])\n",
    "                # print(f\"      Test Set Degraded (only on retrived): Accuracy={acc_test_degraded}, Top-5 Accuracy={top_5_acc_test_degraded}\")\n",
    "                \n",
    "                # # Training su entrambi i set\n",
    "                # knn = KNeighborsClassifier(n_neighbors=k)\n",
    "                # knn.fit(combined_features, combined_labels)\n",
    "\n",
    "                # # Test su test set\n",
    "                # top_5_predictions_test = np.argsort(knn.predict_proba(features_test), axis=1)[:, -5:]\n",
    "                # acc_test = knn.score(features_test, labels_test)\n",
    "                # top_5_acc_test = np.mean([y in top_5 for y, top_5 in zip(labels_test, top_5_predictions_test)])\n",
    "                # logs.append([dim, small_cleaned, dim_cleaned, \"Test Set (on small and retrived)\", k, acc_test, top_5_acc_test])\n",
    "                # print(f\"      Test Set (on small and retrived): Accuracy={acc_test}, Top-5 Accuracy={top_5_acc_test}\")\n",
    "\n",
    "                # # Test su test set degraded\n",
    "                # top_5_predictions_test_degraded = np.argsort(knn.predict_proba(features_test_degraded), axis=1)[:, -5:]\n",
    "                # acc_test_degraded = knn.score(features_test_degraded, labels_test_degraded)\n",
    "                # top_5_acc_test_degraded = np.mean([y in top_5 for y, top_5 in zip(labels_test_degraded, top_5_predictions_test_degraded)])\n",
    "                # logs.append([dim, small_cleaned, dim_cleaned, \"Test Set Degraded (on small and retrived)\", k, acc_test_degraded, top_5_acc_test_degraded])\n",
    "                # print(f\"      Test Set Degraded (on small and retrived): Accuracy={acc_test_degraded}, Top-5 Accuracy={top_5_acc_test_degraded}\")\n",
    "\n",
    "# Salva i log in un CSV\n",
    "log_df = pd.DataFrame(logs, columns=log_columns)\n",
    "log_df.to_csv(log_path, index=False)\n",
    "print(f\"Log salvato in {log_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
